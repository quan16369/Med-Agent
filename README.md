# ğŸ¥ MedAssist: Agentic Medical Graph-RAG with MedGemma

**Competition**: [MedGemma Impact Challenge (Kaggle)](https://www.kaggle.com/competitions/med-gemma-impact-challenge)  
**Category**: Agentic Workflow Prize ($10,000)  
**Deadline**: February 24, 2026  
**Base Architecture**: [AMG-RAG (EMNLP 2025)](https://github.com/MrRezaeiUofT/AMG-RAG)  
**Optimization**: SaraCoder-inspired resource optimization (arXiv:2508.10068)

An intelligent medical question answering system that uses **agentic workflows** to dynamically construct knowledge graphs and reason over medical evidence, enhanced with **hierarchical optimization** for maximum efficiency.

---

## ğŸ¯ Project Overview

MedAssist implements **AMG-RAG (Agentic Medical Graph-RAG)** with Google's MedGemma models:

### What Makes This Agentic?

Traditional RAG: `Query â†’ Retrieve â†’ Generate`

Our Agentic Workflow: 
```
Query â†’ Extract Entities â†’ Search Evidence â†’ Build Knowledge Graph â†’ 
Explore Reasoning Paths â†’ Synthesize Answer
```

Each stage is an **autonomous agent** that makes decisions about what entities to extract, what evidence to retrieve, what graph paths to explore, and how to synthesize the final answer.

### Key Innovation

- **Dynamic Knowledge Graphs**: Built at query-time from medical evidence
- **Path-Based Reasoning**: Explores graph connections for chain-of-thought
- **Multi-Source Evidence**: PubMed, Wikipedia, medical databases
- **MedGemma Integration**: Google's specialized medical AI models

---

## ğŸ—ï¸ Architecture

### 5-Stage Agentic Workflow (LangGraph)

```python
Stage 1: Entity Extraction Agent
â”œâ”€ Input: User query
â”œâ”€ Action: Extract medical entities with 1-10 relevance scores
â””â”€ Output: List of MedicalEntity objects

Stage 2: Evidence Retrieval Agent
â”œâ”€ Input: Extracted entities
â”œâ”€ Action: Search PubMed for each entity
â””â”€ Output: Retrieved articles with abstracts

Stage 3: Knowledge Graph Construction Agent
â”œâ”€ Input: Entities + Evidence
â”œâ”€ Action: Extract relationships, build NetworkX graph
â””â”€ Output: MedicalKnowledgeGraph

Stage 4: Path-Based Reasoning Agent
â”œâ”€ Input: Knowledge graph + Query
â”œâ”€ Action: Explore graph paths between entities
â””â”€ Output: Reasoning paths

Stage 5: Answer Generation Agent
â”œâ”€ Input: Reasoning paths + Evidence
â”œâ”€ Action: Chain-of-thought synthesis
â””â”€ Output: Final answer with confidence
```

### Technology Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **LLM** | MedGemma 3 (8B/27B) | Medical reasoning & entity extraction |
| **Orchestration** | LangGraph | Agentic workflow management |
| **Knowledge Graph** | NetworkX | Multi-directional relationship graph |
| **Evidence** | PubMed API | Peer-reviewed medical literature |
| **Embeddings** | Sentence-Transformers | Semantic similarity |
| **Vector DB** | Chroma (optional) | Efficient retrieval |

---

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone repository
git clone <your-repo-url>
cd MedGemma

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure LLM Provider

Choose one of the following:

**Option A: Google GenAI API (Recommended)**
```bash
export GOOGLE_API_KEY='your-api-key'
```

**Option B: Local Ollama**
```bash
# Install Ollama: https://ollama.ai
ollama pull medgemma
ollama serve
```

**Option C: Vertex AI (GCP)**
```bash
export GOOGLE_CLOUD_PROJECT='your-project-id'
gcloud auth application-default login
```

### 3. Run Example

```bash
# Basic test (no PubMed)
python example_usage.py

# Full test with PubMed
python test_amg_rag.py

# Simple query test
python test_amg_rag.py --simple
```

---

## ğŸ’¡ Usage

### Basic Usage

```python
from medassist import AMG_RAG_System

# Initialize system (auto-detects LLM provider)
system = AMG_RAG_System(
    model_name="medgemma-3-8b",
    temperature=0.0,
    enable_pubmed=True
)

# Ask a medical question
result = system.answer_question(
    "What is the mechanism of action of metformin?"
)

print(result["answer"])
print(f"Entities: {result['entities']}")
print(f"Reasoning paths: {len(result['reasoning_paths'])}")
```

### MEDQA-Style Question

```python
query = """
A 65-year-old woman with rheumatoid arthritis is started on methotrexate.
What supplement should be prescribed with this medication?

A) Calcium
B) Folic acid
C) Vitamin D
D) Iron
"""

result = system.answer_question(query)

# Access knowledge graph
kg_stats = result["metadata"]["kg_stats"]
print(f"KG: {kg_stats['num_entities']} entities, {kg_stats['num_relations']} relations")
```

### Custom Configuration

```python
system = AMG_RAG_System(
    model_name="medgemma-3-27b",      # Larger model
    temperature=0.2,                   # Slight creativity
    pubmed_max_results=10,             # More evidence
    min_entity_relevance=7,            # Stricter filtering
    enable_pubmed=True
)
```

---

## ğŸ“Š Project Structure

```
MedGemma/
â”œâ”€â”€ medassist/
â”‚   â”œâ”€â”€ amg_rag.py              # Main AMG-RAG system with LangGraph
â”‚   â”œâ”€â”€ __init__.py             # Package exports
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ medgemma.py         # MedGemma LLM integration
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ knowledge_graph.py  # NetworkX knowledge graph
â”‚   â”‚   â”œâ”€â”€ chains.py           # LLM chains (entity, relation, summarization)
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ entities.py         # MedicalEntity, MedicalRelation
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ tools/
â”‚       â”œâ”€â”€ pubmed.py           # PubMed API integration
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ test_amg_rag.py             # Full system test
â”œâ”€â”€ test_basic.py               # Unit tests
â”œâ”€â”€ test_pubmed.py              # PubMed integration test
â”œâ”€â”€ example_usage.py            # Usage examples
â”œâ”€â”€ requirements.txt            # Dependencies
â”œâ”€â”€ .env.example                # Environment configuration
â”œâ”€â”€ AMG_RAG_START.md           # Development roadmap
â””â”€â”€ README.md                   # This file
```

---

## ğŸ“ How It Works

### Example Workflow

**Query**: "What is the mechanism of diabetic neuropathy?"

**Stage 1: Entity Extraction**
```
Entities extracted:
- diabetic neuropathy (disease, relevance: 10)
- diabetes mellitus (disease, relevance: 9)
- hyperglycemia (symptom, relevance: 8)
- nerve damage (symptom, relevance: 8)
```

**Stage 2: Evidence Retrieval**
```
PubMed search for "diabetic neuropathy":
- 5 articles retrieved
- Abstracts extracted and parsed
```

**Stage 3: Knowledge Graph**
```
Graph built: 15 entities, 23 relationships
Key relationships:
- hyperglycemia --[causes]--> nerve damage
- diabetes mellitus --[causes]--> hyperglycemia
- hyperglycemia --[leads_to]--> oxidative stress
```

**Stage 4: Path-Based Reasoning**
```
Path 1: diabetes â†’ hyperglycemia â†’ oxidative stress â†’ nerve damage
Path 2: diabetes â†’ advanced glycation â†’ microvascular damage
Path 3: hyperglycemia â†’ polyol pathway â†’ nerve dysfunction
```

**Stage 5: Answer Generation**
```
Chain of Thought:
1. Chronic hyperglycemia is the primary trigger
2. Multiple pathways contribute: polyol, oxidative stress, AGEs
3. Microvascular damage leads to ischemia
4. Result: axonal degeneration and demyelination

Final Answer: Diabetic neuropathy occurs through multiple mechanisms...
Confidence: High
```

---

## ğŸ”¬ Features

### âœ… Implemented

- [x] MedGemma LLM integration (Google GenAI, Vertex AI, Ollama, vLLM)
- [x] **Multimodal support** (X-ray, CT, MRI, histopathology image analysis)
- [x] Entity extraction with 1-10 relevance scoring
- [x] Bidirectional relationship extraction
- [x] NetworkX-based knowledge graph
- [x] PubMed evidence retrieval
- [x] LangGraph agentic workflow
- [x] Path-based reasoning
- [x] Chain-of-thought answer generation
- [x] Multi-provider LLM support (auto-detection)
- [x] Medical image report generation
- [x] Longitudinal image comparison
- [x] **SaraCoder-inspired optimization** (hierarchical entity/evidence optimization)
- [x] **Medical term disambiguation** (context-aware abbreviation resolution)
- [x] **Diversity-optimized retrieval** (MMR for maximum information coverage)

### ğŸš§ In Progress

- [ ] Wikipedia integration
- [ ] Vector database (Chroma) for semantic search
- [ ] MEDQA dataset evaluation (text + image questions)
- [ ] Knowledge graph visualization
- [ ] Semantic embeddings for entity clustering
- [ ] Multimodal RAG with image embeddings (MedSigLIP)

### ğŸ¯ Competition Goals

**Target Category**: Agentic Workflow Prize

**Criteria**:
- âœ… Reimagines complex workflow with intelligent agents
- âœ… Uses HAI-DEF models (MedGemma) effectively
- âœ… Demonstrates significant efficiency improvements
- âœ… Showcases autonomous decision-making

---

## ğŸ“ˆ Performance Targets

Based on AMG-RAG paper (EMNLP 2025):

| Dataset | Target Score | Metric |
|---------|-------------|--------|
| MEDQA | 74%+ | F1 Score |
| MEDMCQA | 66%+ | Accuracy |
| MedQA-USMLE | 70%+ | Accuracy |

---

## ğŸ› ï¸ Development

### Running Tests

```bash
# Unit tests
python test_basic.py

# PubMed integration
python test_pubmed.py

# Full system test
python test_amg_rag.py

# Simple query test
python test_amg_rag.py --simple
```

### Environment Variables

See `.env.example` for configuration options:

```bash
# Required: LLM Provider
GOOGLE_API_KEY=your-key

# Optional: PubMed
PUBMED_API_KEY=your-ncbi-key
PUBMED_EMAIL=your@email.com

# Optional: Logging
LOG_LEVEL=INFO
```

---

## ğŸ“š References

- **AMG-RAG Paper**: [Agentic Medical Graph-RAG (EMNLP 2025)](https://github.com/MrRezaeiUofT/AMG-RAG)
- **HAI-DEF Models**: [Google Health AI Developer Foundations](https://developers.google.com/health-ai-developer-foundations)
- **MedGemma**: [Medical Gemma Models](https://developers.google.com/health-ai-developer-foundations/medgemma)
- **Competition**: [MedGemma Impact Challenge](https://www.kaggle.com/competitions/med-gemma-impact-challenge)

---

## ğŸ“ License

MIT License - See [terms](https://developers.google.com/health-ai-developer-foundations/terms) for HAI-DEF models usage.

---

## ğŸ¤ Contributing

This is a competition entry. After the competition (Feb 24, 2026), contributions welcome!

**Contact**: [Your contact info]  
**Team**: [Your team members]
