# Medical Knowledge Assistant

Production-ready multi-agent workflow system for medical question answering with knowledge graph reasoning.

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)

## Quick Links

- [Deployment Guide](docs/DEPLOYMENT.md) - Complete production deployment documentation
- [Architecture](docs/ARCHITECTURE.md) - System architecture and design
- [MCP Architecture](docs/MCP_ARCHITECTURE.md) - Model Context Protocol implementation
- [Setup Guide](docs/SETUP.md) - Installation and configuration
- [Refactoring History](docs/REFACTORING.md) - Development history

---

## Architecture Overview

Multi-agent workflow system with Model Context Protocol (MCP):

**Agents**:
- **Knowledge Agent**: Queries medical knowledge graph with BFS/DFS traversal
- **Diagnostic Agent**: Analyzes symptoms and provides diagnostic reasoning
- **Treatment Agent**: Recommends evidence-based treatment options
- **Evidence Agent**: Retrieves scientific literature from PubMed
- **Validator Agent**: Cross-validates findings for consistency

**MCP Components**:
- **GraphRAG Ingestion**: Document processing and knowledge extraction
- **MCP Server**: Tools (KG Search, KG Write, Web Search) and Skills (Update Memory, Write Content)
- **MCP Client**: User interface and API access

### Key Features

- Medical knowledge graph with multi-hop reasoning
- BioBERT-based named entity recognition
- Graph-conditioned retrieval with confidence scoring
- Agent collaboration and reflection
- Model Context Protocol for tool orchestration
- Document ingestion pipeline (GraphRAG)
- Production-ready infrastructure (API, monitoring, health checks)

---

## ğŸ“Š Performance

| Metric | AMG-RAG (Paper) | Baseline RAG |
|--------|----------------|--------------|
| MEDQA Accuracy | **73.9%** | 65.6% |
| MEDMCQA Accuracy | **66.3%** | 58.1% |
| F1 Score | **74.1%** | 67.2% |
| Model Size | 8B params | 70B+ params |

**Key Advantage**: Achieves GPT-4 level performance with 10Ã— smaller model

---

## Quick Start

### Docker Compose (Recommended)

```bash
# Clone repository
git clone https://github.com/yourusername/MedGemma.git
cd MedGemma

# Start all services
docker-compose up -d

# Check status
curl http://localhost:8000/health

# Query API
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"question": "What causes diabetes and how is it treated?"}'
```

### Local Development

```bash
# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your settings

# Run API server
python run_server.py

# Or run demo
python examples/demo_agentic.py
```

---

## Usage

### API Endpoints

**POST /query** - Process medical question
```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What are the symptoms of diabetes?",
    "include_trace": true,
    "include_stats": true
  }'
```

**GET /health** - System health check
```bash
curl http://localhost:8000/health
```

**GET /stats** - System statistics
```bash
curl http://localhost:8000/stats
```

**GET /docs** - Interactive API documentation

### Python API

```python
from medassist.agentic_orchestrator import AgenticMedicalOrchestrator

# Initialize orchestrator
orchestrator = AgenticMedicalOrchestrator()

# Process query
result = orchestrator.execute_workflow(
    "What causes diabetes and how is it treated?"
)

print(f"Answer: {result['answer']}")
print(f"Confidence: {result['confidence']:.2f}")
print(f"Agents used: {len(result['statistics'])} agents")
```

---

## Project Structure

```
MedGemma/
â”œâ”€â”€ medassist/              # Core package
â”‚   â”œâ”€â”€ config.py           # Configuration management
â”‚   â”œâ”€â”€ exceptions.py       # Error handling
â”‚   â”œâ”€â”€ logging_utils.py    # Logging utilities
â”‚   â”œâ”€â”€ health.py           # Health checks
â”‚   â”œâ”€â”€ knowledge_graph.py  # Knowledge graph
â”‚   â”œâ”€â”€ graph_retrieval.py  # Graph traversal
â”‚   â”œâ”€â”€ medical_ner.py      # Medical NER
â”‚   â”œâ”€â”€ pubmed_retrieval.py # PubMed API
â”‚   â”œâ”€â”€ agentic_workflow.py # Multi-agent system
â”‚   â””â”€â”€ agentic_orchestrator.py # Main orchestrator
â”œâ”€â”€ tests/                  # Test suite
â”‚   â”œâ”€â”€ test_units.py
â”‚   â”œâ”€â”€ test_integration.py
â”‚   â””â”€â”€ test_api.py
â”œâ”€â”€ examples/               # Demo scripts
â”‚   â”œâ”€â”€ demo_agentic.py
â”‚   â”œâ”€â”€ demo_amg_rag.py
â”‚   â””â”€â”€ test_components.py
â”œâ”€â”€ docs/                   # Documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â”œâ”€â”€ DEPLOYMENT.md
â”‚   â”œâ”€â”€ SETUP.md
â”‚   â””â”€â”€ REFACTORING.md
â”œâ”€â”€ k8s/                    # Kubernetes configs
â”‚   â””â”€â”€ deployment.yaml
â”œâ”€â”€ .github/workflows/      # CI/CD pipelines
â”‚   â””â”€â”€ ci-cd.yml
â”œâ”€â”€ scripts/                # Utility scripts
â”œâ”€â”€ api.py                  # FastAPI application
â”œâ”€â”€ run_server.py           # Production server
â”œâ”€â”€ Dockerfile              # Container definition
â”œâ”€â”€ docker-compose.yml      # Multi-container setup
â”œâ”€â”€ requirements.txt        # Dependencies
â””â”€â”€ README.md               # This file
```

---

## Testing

```bash
# Run all tests
pytest

# Run specific test suites
pytest tests/test_units.py          # Unit tests
pytest tests/test_integration.py    # Integration tests
pytest tests/test_api.py            # API tests

# With coverage
pytest --cov=medassist --cov-report=html

# Skip slow tests
pytest -m "not slow"
```

---

## Deployment

See [docs/DEPLOYMENT.md](docs/DEPLOYMENT.md) for complete deployment guide.

### Docker

```bash
docker build -t medassist:latest .
docker run -d -p 8000:8000 --env-file .env medassist:latest
```

### Kubernetes

```bash
kubectl apply -f k8s/deployment.yaml
kubectl get pods -n medassist
```

---

## Configuration

Environment variables (see [.env.example](.env.example)):

```bash
# Database
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_password

# Model
DEVICE=cuda  # or cpu
MAX_DEPTH=3
MAX_WIDTH=10

# API
PUBMED_EMAIL=your@email.com
LOG_LEVEL=INFO
```

---

## Contributing

1. Fork the repository
2. Create feature branch
3. Add tests for new features
4. Ensure all tests pass
5. Submit pull request

## License

MIT License - see [LICENSE](LICENSE) file for details.

## Contact

For issues and questions, please open a GitHub issue.

- **Confidence Propagation**: Aggregates uncertainty across hops

### 4. Evidence Retrieval

- **PubMed Search**: Retrieves supporting scientific literature
- **Relevance Ranking**: Scores articles by query relevance
- **Citation Tracking**: Links evidence to knowledge graph edges

---

## ğŸ”¬ Research Foundation

Based on EMNLP 2025 paper:

**"Agentic Medical Knowledge Graphs Enhance Medical Question Answering: Bridging the Gap Between LLMs and Evolving Medical Knowledge"**

*Authors*: Mohammad Reza Rezaei, Reza Saadati Fard, Jayson L. Parker, Rahul G. Krishnan, Milad Lankarany

*Key Contributions*:
- Dynamic knowledge graphs that continuously update from latest research
- Confidence-scored relationships with evidence tracking
- Multi-hop reasoning outperforms flat retrieval by 8-10%
- Achieves 74.1% F1 with only 8B parameters (vs 70B+ baselines)

**Paper**: [EMNLP 2025 Findings #679](https://aclanthology.org/2025.findings-emnlp.679/)

---

## ğŸ§ª Benchmarking

### MEDQA Dataset

```bash
# Download MEDQA test set
mkdir -p data/medqa
wget https://github.com/jind11/MedQA/raw/master/data_clean/questions/US/test.jsonl -P data/medqa/

# Run benchmark
python scripts/benchmark_medqa.py
```

### Expected Results

- **MEDQA Accuracy**: 73.9% (target from paper)
- **MEDMCQA Accuracy**: 66.3%
- **F1 Score**: 74.1%
- **Processing Time**: <1s per query

---

## ğŸ› ï¸ Development

### Requirements

- Python 3.9+
- Neo4j 5.0+ (optional, for production)
- 16GB RAM (recommended)
- CUDA GPU (optional, for BioBERT)

### Dependencies

```
torch>=2.0.0
transformers>=4.40.0
neo4j>=5.0.0
biopython>=1.81
sentence-transformers>=2.5.0
faiss-cpu>=1.8.0
```

### Testing

```bash
# Run all tests
pytest tests/ -v

# Test specific component
pytest tests/test_knowledge_graph.py -v
```

---

## ğŸ“š Documentation

- [AMG_RAG_ARCHITECTURE.md](AMG_RAG_ARCHITECTURE.md) - Detailed architecture documentation
- [AMG_RAG_SETUP.md](AMG_RAG_SETUP.md) - Complete setup and deployment guide
- [2025.findings-emnlp.679.pdf](2025.findings-emnlp.679.pdf) - Original research paper

---

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the Apache License 2.0 - see [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **EMNLP 2025 Paper Authors**: Rezaei et al. for the AMG-RAG architecture
- **BioBERT Team**: DMIS Lab for medical NER models
- **Neo4j**: Graph database platform
- **PubMed/NCBI**: Medical literature database
- **Hugging Face**: Transformers library

---

## ğŸ“ Contact

For questions or issues:
- Open an [issue](https://github.com/yourusername/MedGemma/issues)
- Email: your.email@example.com

---

## â­ Citation

If you use this code in your research, please cite:

```bibtex
@inproceedings{rezaei2025amgrag,
  title={Agentic Medical Knowledge Graphs Enhance Medical Question Answering: Bridging the Gap Between LLMs and Evolving Medical Knowledge},
  author={Rezaei, Mohammad Reza and Fard, Reza Saadati and Parker, Jayson L and Krishnan, Rahul G and Lankarany, Milad},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2025},
  year={2025}
}
```

---

**Built with â¤ï¸ for advancing medical AI**
